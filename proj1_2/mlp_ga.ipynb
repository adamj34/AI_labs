{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prezentacja problemu\n",
    "\n",
    "Celem projektu jest zbadanie, czy algorytmy genetyczne mogą posłużyć do dobrania optymalnych hiperparametrów dla sieci neuronowej. W przypadku tego projektu sieć ma postać perceptronu wielowarstwowego składającego się z jednej warstwy wejściowej (embedding layer), kilku warstw ukrytych i normalizacyjnych oraz jednej warstwy wyjściowej. \\\n",
    "Przykładowa architektura może wyglądać np. tak:\n",
    "\n",
    "nn.Embedding(vocab_size, n_embd), nn.Flatten() \\\n",
    "nn.Linear(n_embd * block_size, n_hidden), norm_layer(n_hidden), activation_func \\\n",
    "nn.Linear(n_hidden, n_hidden),            norm_layer(n_hidden), activation_func \\\n",
    "nn.Linear(n_hidden, n_hidden),            norm_layer(n_hidden), activation_func \\\n",
    "nn.Linear(n_hidden, n_hidden),            norm_layer(n_hidden), activation_func \\\n",
    "nn.Linear(n_hidden, vocab_size),          norm_layer(vocab_size) \\\n",
    "nn.Softmax(vocab_size) \n",
    "\n",
    "Wszystkie modele zostały wytrenowane i przetestowane na zbiorze 32033 imion. Zadaniem perceptronu jest przewidzenie kolejnego znaku w sekwencji na podstawie trzech poprzednich tokenów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "import pygad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "str_to_inx = {str:inx for inx, str in enumerate(chars, start=1)}\n",
    "str_to_inx['.'] = 0\n",
    "inx_to_str = {str:inx for inx, str in str_to_inx.items()}\n",
    "vocab_size = len(inx_to_str)\n",
    "\n",
    "# build the dataset\n",
    "random.shuffle(words)\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for word in words:\n",
    "    word = block_size * '.' + word + '.'\n",
    "    end_inx = block_size\n",
    "    for start_inx, char in enumerate(word[block_size:]):\n",
    "        X.append([str_to_inx[ch] for ch in word[start_inx:end_inx]])\n",
    "        Y.append(str_to_inx[char])\n",
    "        end_inx += 1\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "X, Y = torch.tensor(X).to(device), torch.tensor(Y).to(device)\n",
    "\n",
    "# data set splits 80%, 10%, 10%\n",
    "train_range = math.ceil(len(X) * 0.8)\n",
    "dev_range = (len(X) - train_range) // 2\n",
    "\n",
    "training_set = X[:train_range]\n",
    "dev_set = X[train_range:train_range+dev_range]\n",
    "test_set = X[train_range+dev_range:]\n",
    "\n",
    "y_training_set = Y[:train_range]\n",
    "y_dev_set = Y[train_range:train_range+dev_range]\n",
    "y_test_set = Y[train_range+dev_range:]\n",
    "\n",
    "assert training_set.nelement() + dev_set.nelement() + test_set.nelement() == X.nelement(), \"Bad split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # disable gradient tracking\n",
    "def get_loss(model, data_set):\n",
    "    model.eval()\n",
    "    x, y = {\n",
    "        'train': (training_set, y_training_set),\n",
    "        'dev': (dev_set, y_dev_set),\n",
    "        'test': (test_set, y_test_set)\n",
    "    }[data_set]\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_generation(ga_instance):\n",
    "    print(f\"Generation = {ga_instance.generations_completed}\")\n",
    "    print(f\"Fitness    = {ga_instance.best_solution()[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP0(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP5(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP6(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP7(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP8(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "inputs = [MLP0, MLP1, MLP2, MLP3, MLP4, MLP5, MLP6, MLP7, MLP8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings\n",
    "optim_map = {\n",
    "    0: torch.optim.Adam,\n",
    "    1: torch.optim.SGD,\n",
    "}\n",
    "\n",
    "act_map = {\n",
    "    0: torch.nn.ReLU(),\n",
    "    1: torch.nn.Tanh(),\n",
    "    2: torch.nn.LeakyReLU(),\n",
    "    3: torch.nn.Sigmoid(),\n",
    "}\n",
    "\n",
    "norm_map = {\n",
    "    0: torch.nn.BatchNorm1d,\n",
    "    1: torch.nn.LayerNorm,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_func_factory(inp_model):\n",
    "    def fitness_func(solution, sol_idx):\n",
    "        n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = solution\n",
    "        activation = act_map[activation_num]\n",
    "        norm_layer = norm_map[norm_layer_num]\n",
    "        \n",
    "        model = inp_model(vocab_size, int(n_embd), int(n_hidden), norm_layer, activation).to(device)\n",
    "\n",
    "        optimizer = optim_map[optimizer_num](params=model.parameters(), lr=lr)\n",
    "\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        g = torch.Generator().manual_seed(2147483647)\n",
    "        steps = 750\n",
    "        for _ in range(steps):\n",
    "            inx = torch.randint(0, training_set.shape[0], (int(batch_size),), generator=g)\n",
    "            emb = training_set[inx] # grab only those rows from the minibatch \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            preds = model(emb)\n",
    "            loss = loss_function(preds, y_training_set[inx])\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "        dev_loss = get_loss(model, 'dev')\n",
    "        return 1.0 / (dev_loss + 1e-8)\n",
    "    \n",
    "    return fitness_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m\n\u001b[1;32m     21\u001b[0m ga_instance \u001b[39m=\u001b[39m pygad\u001b[39m.\u001b[39mGA(num_generations\u001b[39m=\u001b[39mnum_generations,\n\u001b[1;32m     22\u001b[0m                     num_parents_mating\u001b[39m=\u001b[39mnum_parents_mating,\n\u001b[1;32m     23\u001b[0m                     num_genes\u001b[39m=\u001b[39mnum_genes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     on_generation\u001b[39m=\u001b[39mcallback_generation,\n\u001b[1;32m     32\u001b[0m                     )\n\u001b[1;32m     34\u001b[0m \u001b[39m# Start the genetic algorithm evolution.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m ga_instance\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     37\u001b[0m ga_instance\u001b[39m.\u001b[39mplot_fitness(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPyGAD & PyTorch - Iteration vs. Fitness\u001b[39m\u001b[39m\"\u001b[39m, linewidth\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m);\n\u001b[1;32m     38\u001b[0m solution \u001b[39m=\u001b[39m ga_instance\u001b[39m.\u001b[39mbest_solution()[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pygad/pygad.py:1582\u001b[0m, in \u001b[0;36mGA.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe type of the iterable holding the selected parents\u001b[39m\u001b[39m'\u001b[39m\u001b[39m indices is expected to be (numpy.ndarray) but \u001b[39m\u001b[39m{last_generation_parents_indices_type}\u001b[39;00m\u001b[39m found.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(last_generation_parents_indices_type\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_generation_parents_indices)))\n\u001b[1;32m   1581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_generation_parents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_generation_parents_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect_parents(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_generation_fitness, num_parents\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_parents_mating)\n\u001b[1;32m   1584\u001b[0m \u001b[39m# Validate the output of the parent selection step: self.select_parents()\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_generation_parents\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_parents_mating, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_genes):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pygad/pygad.py:1847\u001b[0m, in \u001b[0;36mGA.roulette_wheel_selection\u001b[0;34m(self, fitness, num_parents)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mZeroDivisionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot proceed because the sum of fitness values is zero. Cannot divide by zero.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1846\u001b[0m probs \u001b[39m=\u001b[39m fitness \u001b[39m/\u001b[39m fitness_sum\n\u001b[0;32m-> 1847\u001b[0m probs_start \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mzeros(probs\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39;49mfloat) \u001b[39m# An array holding the start values of the ranges of probabilities.\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m probs_end \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mzeros(probs\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mfloat) \u001b[39m# An array holding the end values of the ranges of probabilities.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m curr \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[39m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "num_generations = 2 # Number of generations.\n",
    "num_parents_mating = 5 # Number of solutions to be selected as parents in the mating pool.\n",
    "sol_per_pop = 10\n",
    "mutation_num_genes = 1\n",
    "keep_elitism = 2\n",
    "parent_selection_type = \"rws\" # Type of parent selection.\n",
    "crossover_type = \"single_point\"\n",
    "gene_space = [  \n",
    "                [8, 16, 32, 64],\n",
    "                {'low': 16, 'high': 100, 'step': 1},\n",
    "                [0,1],\n",
    "                [0,1,2],\n",
    "                np.linspace(0.0001, 0.1, 10),\n",
    "                [16, 32, 64],\n",
    "                [0,1],\n",
    "            ]\n",
    "num_genes = len(gene_space)\n",
    "\n",
    "for curr_model in inputs:\n",
    "\n",
    "    ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                        num_parents_mating=num_parents_mating,\n",
    "                        num_genes=num_genes,\n",
    "                        sol_per_pop=sol_per_pop,\n",
    "                        keep_elitism=keep_elitism,\n",
    "                        parent_selection_type=parent_selection_type,\n",
    "                        fitness_func=fitness_func_factory(curr_model),\n",
    "                        crossover_type=crossover_type,\n",
    "                        gene_space=gene_space,\n",
    "                        mutation_num_genes=mutation_num_genes,\n",
    "                        on_generation=callback_generation,\n",
    "                        )\n",
    "\n",
    "    # Start the genetic algorithm evolution.\n",
    "    ga_instance.run()\n",
    "\n",
    "    ga_instance.plot_fitness(title=\"PyGAD & PyTorch - Iteration vs. Fitness\", linewidth=4);\n",
    "    solution = ga_instance.best_solution()[0]\n",
    "    n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = solution\n",
    "    activation = act_map[activation_num]\n",
    "    norm_layer = norm_map[norm_layer_num]\n",
    "    print(f\"Best solution: \\\n",
    "        n_embd = {n_embd},\\\n",
    "        n_hidden_1 = {n_hidden},\\\n",
    "        norm_layer = {norm_map[norm_layer_num]},\\\n",
    "        activation = {act_map[activation_num]},\\\n",
    "        lr = {lr},\\\n",
    "        batch_size = {batch_size},\\\n",
    "        optimizer = {optim_map[optimizer_num]},\\\n",
    "    \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
