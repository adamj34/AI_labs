{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prezentacja problemu\n",
    "\n",
    "Celem projektu jest zbadanie, czy algorytmy genetyczne mogą posłużyć do dobrania optymalnych hiperparametrów dla sieci neuronowej. W przypadku tego projektu sieć ma postać perceptronu wielowarstwowego składającego się z jednej warstwy wejściowej (embedding layer), kilku warstw ukrytych i normalizacyjnych oraz jednej warstwy wyjściowej. \\\n",
    "Przykładowa architektura może wyglądać np. tak:\n",
    "\n",
    "nn.Embedding(vocab_size, n_embd), nn.Flatten() \\\n",
    "nn.Linear(n_embd * block_size, n_hidden, bias), norm_layer(n_hidden), activation_func \\\n",
    "nn.Linear(n_hidden, n_hidden, bias),            norm_layer(n_hidden), activation_func \\\n",
    "nn.Linear(n_hidden, n_hidden, bias),            norm_layer(n_hidden), activation_func \\\n",
    "nn.Linear(n_hidden, n_hidden, bias),            norm_layer(n_hidden), activation_func \\\n",
    "nn.Linear(n_hidden, vocab_size, bias),          norm_layer(vocab_size) \\\n",
    "nn.Softmax(vocab_size) \n",
    "\n",
    "Wszystkie modele zostały wytrenowane i przetestowane na zbiorze 32033 imion. Zadaniem perceptronu jest przewidzenie kolejnego znaku w sekwencji na podstawie trzech poprzednich tokenów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "import pygad\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowanie danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "str_to_inx = {str:inx for inx, str in enumerate(chars, start=1)}\n",
    "str_to_inx['.'] = 0\n",
    "inx_to_str = {str:inx for inx, str in str_to_inx.items()}\n",
    "vocab_size = len(inx_to_str)\n",
    "\n",
    "# build the dataset\n",
    "random.shuffle(words)\n",
    "block_size = 3 # context length\n",
    "X, Y = [], []\n",
    "for word in words:\n",
    "    word = block_size * '.' + word + '.'\n",
    "    end_inx = block_size\n",
    "    for start_inx, char in enumerate(word[block_size:]):\n",
    "        X.append([str_to_inx[ch] for ch in word[start_inx:end_inx]])\n",
    "        Y.append(str_to_inx[char])\n",
    "        end_inx += 1\n",
    "\n",
    "\n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "# data set splits: train 80%, dev 10%, test 10%\n",
    "train_range = math.ceil(len(X) * 0.8)\n",
    "dev_range = (len(X) - train_range) // 2\n",
    "\n",
    "training_set = X[:train_range]\n",
    "dev_set = X[train_range:train_range+dev_range]\n",
    "test_set = X[train_range+dev_range:]\n",
    "\n",
    "y_training_set = Y[:train_range]\n",
    "y_dev_set = Y[train_range:train_range+dev_range]\n",
    "y_test_set = Y[train_range+dev_range:]\n",
    "\n",
    "assert training_set.nelement() + dev_set.nelement() + test_set.nelement() == X.nelement(), \"Bad split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # disable gradient tracking\n",
    "def get_loss(model, data_set):\n",
    "    model.eval()\n",
    "    x, y = {\n",
    "        'train': (training_set, y_training_set),\n",
    "        'dev': (dev_set, y_dev_set),\n",
    "        'test': (test_set, y_test_set)\n",
    "    }[data_set]\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputami w tym eksperymencie będzie 9 modeli o różnej liczbie warst ukrytych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trzy inputy duże \n",
    "class MLP0(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "# trzy inputy średnie\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP5(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "#trzy inputy małe\n",
    "class MLP6(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP7(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class MLP8(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "inputs = [MLP0, MLP1, MLP2, MLP3, MLP4, MLP5, MLP6, MLP7, MLP8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperyment 1\n",
    "\n",
    "Geny znajdujące się w chromosomie:\n",
    "* **n_embd** - długość vector embedding (wektorów wstawieniowych???) dla pojedyńczego tokenu. \\\n",
    "Możliwe wartości: \n",
    "    * 8\n",
    "    * 16\n",
    "    * 32\n",
    "* **n_hidden** - ilość neuronów w warstwie ukrytej. \\\n",
    "Możliwe wartości:\n",
    "    * dowolna liczba całkowita z przedziału [16, 80]\n",
    "* **norm_layer** - typ warstwy normalizacyjnej. \\\n",
    "Możliwe wartości:\n",
    "    * BatchNorm\n",
    "    * LayerNorm\n",
    "* **activation_func** - funkcja aktywacji. \\\n",
    "Możliwe wartości:\n",
    "    * ReLU\n",
    "    * Tanh\n",
    "    * LeakyReLU\n",
    "    * Sigmoid\n",
    "* **lr** - wspólczynnik uczenia. \\\n",
    "Możliwe wartości:\n",
    "    * 0.0001\n",
    "    * 0.001\n",
    "    * 0.005\n",
    "    * 0.01\n",
    "    * 0.1\n",
    "* **batch_size** - wielkość batcha. \\\n",
    "Możliwe wartości:\n",
    "    * 16\n",
    "    * 32\n",
    "    * 64\n",
    "* **optimizer** - optymalizator. \\\n",
    "Możliwe wartości:\n",
    "    * Adam\n",
    "    * SGD\n",
    "\n",
    "Przykładowy chromosom może wyglądać następująco: \\\n",
    "| n_embd | n_hidden | norm_layer | activation_func | lr | batch_size | optimizer | \n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "|32 | 53 | BatchNorm | Tanh | 0.01 | 64 | SDG |  \n",
    "\n",
    "\\*wartości nieliczbowe w programie są zakodowane jako liczby całkowite\n",
    "\n",
    "#### Funkcja Fitness\n",
    "\n",
    "Funkcja fitness tworzy model o hiperparametrach podanych w chromosomie i trenuje stworzony model przez 1 epokę. Następnie obliczana jest wartość funkcji straty (przy użyciu funkcji CrossEntropy). Funkcja fitness zwraca wartość postaci: \n",
    "* **1.0 / (loss(*zbiór ewaluacyjny*) + 1e-8)** \n",
    "\n",
    "W tym przypadku dążymy do tego aby loss(zbiór ewaluacyjny) był jak najmniejszy, ale funkcja fitness szuka maksimum. Z tego powodu 1.0 musi znaleźć się w liczniku. Zbiór wartości funkcji to \\(0, 100000000\\]\n",
    "\n",
    "#### Parametry i działanie algorytmu genetycznego\n",
    "\n",
    "* num_generations = 15 \n",
    "* num_parents_mating = 4\n",
    "* sol_per_pop = 6\n",
    "* mutation_num_genes = 1\n",
    "* keep_elitism = 2\n",
    "* parent_selection_type = \"rws\" \n",
    "* crossover_type = \"single_point\"\n",
    "\n",
    "W każdej generacji wybierane są dwa chromosomy o najlepszym fitnessie, które na pewno znajdą się w kolejnej populacji. Następnie, rodzice wybierani są przy pomocy algorytmu \"rws\" (tzn. im lepszą wartość funkcji fitness ma rodzic, tym większe są jego szanse na zostanie wybranym). Krzyżowanie zachodzi w jednym, losowo wybranym punkcie chromosomu aż do uzyskania wymaganej liczby członków populacji. Losowa mutacja została ustawiona na jeden gen.\n",
    "Wielkość generacji wynosi 15 (w przypadku dostępu do większych mocy obliczeniowych ta wartość powinna być większa). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings - encoding the hyperparameters as integers\n",
    "optim_map = {\n",
    "    0: torch.optim.Adam,\n",
    "    1: torch.optim.SGD,\n",
    "}\n",
    "\n",
    "act_map = {\n",
    "    0: torch.nn.ReLU(),\n",
    "    1: torch.nn.Tanh(),\n",
    "    2: torch.nn.LeakyReLU(),\n",
    "    3: torch.nn.Sigmoid(),\n",
    "}\n",
    "\n",
    "norm_map = {\n",
    "    0: torch.nn.BatchNorm1d,\n",
    "    1: torch.nn.LayerNorm,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_func_factory(inp_model):\n",
    "    def fitness_func(solution, sol_idx):\n",
    "        n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = solution\n",
    "        bias = True if int(norm_layer_num) == 1 else False\n",
    "        activation = act_map[activation_num]\n",
    "        norm_layer = norm_map[norm_layer_num]\n",
    "        \n",
    "        model = inp_model(vocab_size, int(n_embd), int(n_hidden), bias, norm_layer, activation)\n",
    "\n",
    "        optimizer = optim_map[optimizer_num](params=model.parameters(), lr=lr)\n",
    "\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        g = torch.Generator().manual_seed(2147483647)\n",
    "        steps = 5704\n",
    "        for _ in range(steps):\n",
    "            inx = torch.randint(0, training_set.shape[0], (int(batch_size),), generator=g)\n",
    "            emb = training_set[inx] # grab only those rows from the minibatch \n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # forward pass\n",
    "            preds = model(emb)\n",
    "            loss = loss_function(preds, y_training_set[inx])\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "        dev_loss = get_loss(model, 'dev')\n",
    "        return 1.0 / (dev_loss + 1e-8)\n",
    "    \n",
    "    return fitness_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generations = 15 # Number of generations.\n",
    "num_parents_mating = 4 # Number of solutions to be selected as parents in the mating pool.\n",
    "sol_per_pop = 6\n",
    "mutation_num_genes = 1\n",
    "keep_elitism = 2\n",
    "parent_selection_type = \"rws\" # Type of parent selection.\n",
    "crossover_type = \"single_point\"\n",
    "gene_space = [  \n",
    "                [8, 16, 32],\n",
    "                {'low': 16, 'high': 80, 'step': 1},\n",
    "                [0, 1],\n",
    "                [0, 1, 2, 3],\n",
    "                [0.0001, 0.001, 0.005, 0.01, 0.1],\n",
    "                [16, 32, 64],\n",
    "                [0, 1],\n",
    "            ]\n",
    "num_genes = len(gene_space)\n",
    "\n",
    "best_fitnesses = []\n",
    "for i in range(len(inputs)):\n",
    "\n",
    "    ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                        num_parents_mating=num_parents_mating,\n",
    "                        num_genes=num_genes,\n",
    "                        sol_per_pop=sol_per_pop,\n",
    "                        keep_elitism=keep_elitism,\n",
    "                        parent_selection_type=parent_selection_type,\n",
    "                        fitness_func=fitness_func_factory(inputs[i]),\n",
    "                        crossover_type=crossover_type,\n",
    "                        gene_space=gene_space,\n",
    "                        mutation_num_genes=mutation_num_genes,\n",
    "                        )\n",
    "\n",
    "    # Start the genetic algorithm evolution.\n",
    "    ga_instance.run()\n",
    "\n",
    "    ga_instance.plot_fitness(title=\"PyGAD & PyTorch - Iteration vs. Fitness\", linewidth=4);\n",
    "    solution, best_fitness, _ = ga_instance.best_solution()\n",
    "    n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = solution\n",
    "    activation = act_map[activation_num]\n",
    "    norm_layer = norm_map[norm_layer_num]\n",
    "    best_fitnesses.append(best_fitness)\n",
    "    \n",
    "    print(f\"Best solution of MLP{i} - number of hidden layers: {10-i}: \\n\"\n",
    "      f\"n_embd = {n_embd}\\n\"\n",
    "      f\"n_hidden = {n_hidden}\\n\"\n",
    "      f\"norm_layer = {norm_map[norm_layer_num]}\\n\"\n",
    "      f\"activation = {act_map[activation_num]}\\n\"\n",
    "      f\"lr = {lr} \\n\"\n",
    "      f\"batch_size = {batch_size}\\n\"\n",
    "      f\"optimizer = {optim_map[optimizer_num]}\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| MLP0 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP1 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP2 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP3 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP4 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP5 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP6 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP7 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP8 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprawdzenie efektywności algorytmu genetycznego\n",
    "\n",
    "Badanie efektywności algorytmu genetycznego wedle kryteriów zaprponowanych w opisie projektu jest trochę problematyczne, ponieważ najefektywniejsze rozwiązanie do którego algorytm miałby dążyć nie jest znane. \n",
    "\n",
    "Zamiast tego porównam model o najlepszej funkcji fitness z modelem o losowo dobranych hiperparametrach. Porówne zostaną trzy modele, po jednym z każdej kategorii wielkości inputów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_fit_pairs = list(zip(inputs, best_fitnesses))\n",
    "for i in range(len(inputs)):\n",
    "    if i == 0:\n",
    "        print(\"Small Inputs:\")\n",
    "    elif i == 3:\n",
    "        print(\"Medium Inputs:\")\n",
    "    elif i == 6:\n",
    "        print(\"Large Inputs:\")\n",
    "    print(f\"    MLP{i} - fitness: {inp_fit_pairs[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hiperparams():\n",
    "    n_embd = np.random.choice([8, 16, 32])\n",
    "    n_hidden = np.random.choice(range(16, 80, 1))\n",
    "    norm_layer_num = np.random.choice([0, 1])\n",
    "    activation_num = np.random.choice([0, 1, 2, 3])\n",
    "    lr = np.random.choice([0.0001, 0.001, 0.005, 0.01, 0.1])\n",
    "    batch_size = np.random.choice([16, 32, 64])\n",
    "    optimizer_num = np.random.choice([0, 1])\n",
    "    return n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, model):\n",
    "    activation = act_map[activation_num]\n",
    "    norm_layer = norm_map[norm_layer_num]\n",
    "    bias = True if int(norm_layer_num) == 1 else False\n",
    "    model = model(vocab_size, n_embd, n_hidden, bias, norm_layer, activation)\n",
    "    optimizer = optim_map[optimizer_num](params=model.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "    step_arr, dev_loss_arr = [], []\n",
    "    steps = 15000\n",
    "    start = time.time()\n",
    "    for step in range(steps):\n",
    "        if step == 8000:\n",
    "            optimizer.param_groups[0]['lr'] = lr * 0.7\n",
    "        elif step == 13000:\n",
    "            optimizer.param_groups[0]['lr'] = lr * 0.5\n",
    "\n",
    "        inx = torch.randint(0, training_set.shape[0], (int(batch_size),), generator=g)\n",
    "        emb = training_set[inx] # grab only those rows from the minibatch \n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # forward pass\n",
    "        preds = model(emb)\n",
    "        loss = loss_function(preds, y_training_set[inx])\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #stats\n",
    "        if step % 250 == 0:\n",
    "            dev_loss = get_loss(model, 'dev')\n",
    "            dev_loss_arr.append(dev_loss)\n",
    "            step_arr.append(step)\n",
    "\n",
    "    end = time.time()\n",
    "    training_time = round(end - start, 2)\n",
    "\n",
    "    return training_time, step_arr, dev_loss_arr\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mały input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallRandomMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = draw_hiperparams()\n",
    "rand_train_time, rand_step, rand_loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, SmallRandomMLP)\n",
    "\n",
    "train_time, step, loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, MLP7)\n",
    "\n",
    "print(f\"Random MLP training time: {rand_train_time} seconds\")\n",
    "print(f\"MLP7 training time: {train_time} seconds\")\n",
    "plt.plot(rand_step, rand_loss, label='Random MLP');\n",
    "plt.plot(step, loss, label='MLP7');\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Dev Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Średni input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumRandomMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = draw_hiperparams()\n",
    "rand_train_time, rand_step, rand_loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, MediumRandomMLP)\n",
    "\n",
    "train_time, step, loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, MLP4)\n",
    " \n",
    "print(f\"Random MLP training time: {rand_train_time} seconds\")\n",
    "print(f\"MLP4 training time: {train_time} seconds\")\n",
    "plt.plot(rand_step, rand_loss, label='Random MLP');\n",
    "plt.plot(step, loss, label='MLP4');\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Dev Loss')\n",
    "plt.legend();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duży input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigRandomMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = draw_hiperparams()\n",
    "rand_train_time, rand_step, rand_loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, BigRandomMLP)\n",
    "\n",
    "train_time, step, loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, MLP1)\n",
    "\n",
    "print(f\"Random MLP training time: {rand_train_time} seconds\")\n",
    "print(f\"MLP1 training time: {train_time} seconds\")\n",
    "plt.plot(rand_step, rand_loss, label='Random MLP');\n",
    "plt.plot(step, loss, label='MLP1');\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Dev Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski z Eksperymentu1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperyment 2\n",
    "\n",
    "Zmiany w stosunku do algorytmu z Eksperymentu 1:\n",
    "* parent_selection_type = \"rws\" ---> parent_selection_type = \"sss\"\n",
    "* crossover_type = \"single_point\" ---> crossover_type = \"two_points\"\n",
    "* z grona funkcji aktywacji usunięto sigmoid\n",
    "* funkcja fitness uległa małej modyfikacji. Tym razem zwracana jest wartość obliczna ze wzoru: \n",
    "    * **1.0 / (loss(*zbiór ewaluacyjny*) + (0.001 * training_time))** \\\n",
    "    , gdzie training time to czas przez jaki model był trenowany.\n",
    "\n",
    "W przypadku algorytmu z Eksperymentu 1 każdy model jest trenowany przez zaledwie 1 epokę (jest to spowodowane ograniczeniami czasowymi). Oznacza to, że modele raczej nie mają szansy na znalezienia globalnego minimum i de facto oceniane są jedynie na podstawie tego, jak szybko są w stanie osiągnąć jak najmniejszą wartość dla funkcji straty w ciągu 1 epoki. Faworyzuje to model o jak większej ilości parametrów i jak największym współczynniku uczenia, ale taki model wcale nie musi być najlepszy w ostatecznym rozrachunku. W celu przeciwdziałania temu dodano parametr training_time, który powinien w jakimś stopniu penalizować większe modele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_func_factory(inp_model):\n",
    "    def fitness_func(solution, sol_idx):\n",
    "        n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = solution\n",
    "        bias = True if int(norm_layer_num) == 1 else False\n",
    "        activation = act_map[activation_num]\n",
    "        norm_layer = norm_map[norm_layer_num]\n",
    "        \n",
    "        model = inp_model(vocab_size, int(n_embd), int(n_hidden), bias, norm_layer, activation)\n",
    "\n",
    "        optimizer = optim_map[optimizer_num](params=model.parameters(), lr=lr)\n",
    "\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        g = torch.Generator().manual_seed(2147483647)\n",
    "        steps = 5704\n",
    "        start = time.time()\n",
    "        for _ in range(steps):\n",
    "            inx = torch.randint(0, training_set.shape[0], (int(batch_size),), generator=g)\n",
    "            emb = training_set[inx] # grab only those rows from the minibatch \n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # forward pass\n",
    "            preds = model(emb)\n",
    "            loss = loss_function(preds, y_training_set[inx])\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "        end = time.time()\n",
    "        training_time = round(end - start, 2)\n",
    "        dev_loss = get_loss(model, 'dev')\n",
    "        return 1.0 / (dev_loss + (0.001 * training_time))\n",
    "    \n",
    "    return fitness_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generations = 15 # Number of generations.\n",
    "num_parents_mating = 4 # Number of solutions to be selected as parents in the mating pool.\n",
    "sol_per_pop = 6\n",
    "mutation_num_genes = 1\n",
    "parent_selection_type = \"sss\" # Type of parent selection.\n",
    "crossover_type = \"two_points\"\n",
    "gene_space = [  \n",
    "                [8, 16, 32],\n",
    "                {'low': 16, 'high': 80, 'step': 1},\n",
    "                [0, 1],\n",
    "                [0, 1, 2],\n",
    "                [0.0001, 0.001, 0.005, 0.01, 0.1],\n",
    "                [16, 32, 64],\n",
    "                [0, 1],\n",
    "            ]\n",
    "num_genes = len(gene_space)\n",
    "\n",
    "best_fitnesses = []\n",
    "for i in range(len(inputs)):\n",
    "\n",
    "    ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                        num_parents_mating=num_parents_mating,\n",
    "                        num_genes=num_genes,\n",
    "                        sol_per_pop=sol_per_pop,\n",
    "                        parent_selection_type=parent_selection_type,\n",
    "                        fitness_func=fitness_func_factory(inputs[i]),\n",
    "                        crossover_type=crossover_type,\n",
    "                        gene_space=gene_space,\n",
    "                        mutation_num_genes=mutation_num_genes,\n",
    "                        )\n",
    "\n",
    "    # Start the genetic algorithm evolution.\n",
    "    ga_instance.run()\n",
    "\n",
    "    ga_instance.plot_fitness(title=\"PyGAD & PyTorch - Iteration vs. Fitness\", linewidth=4);\n",
    "    solution, best_fitness, _ = ga_instance.best_solution()\n",
    "    n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = solution\n",
    "    activation = act_map[activation_num]\n",
    "    norm_layer = norm_map[norm_layer_num]\n",
    "    best_fitnesses.append(best_fitness)\n",
    "    \n",
    "    print(f\"Best solution of MLP{i} - number of hidden layers: {10-i}: \\n\"\n",
    "      f\"n_embd = {n_embd}\\n\"\n",
    "      f\"n_hidden = {n_hidden}\\n\"\n",
    "      f\"norm_layer = {norm_map[norm_layer_num]}\\n\"\n",
    "      f\"activation = {act_map[activation_num]}\\n\"\n",
    "      f\"lr = {lr}\\n\"\n",
    "      f\"batch_size = {batch_size}\\n\"\n",
    "      f\"optimizer = {optim_map[optimizer_num]}\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| MLP0 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP1 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP2 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP3 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP4 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP5 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP6 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP7 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |\n",
    "| MLP8 | n_embd | n_hidden | norm_layer_num | activation_num | lr | batch_size | optimizer_num |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie efektywności algorytmu genetycznego\n",
    "\n",
    "Metodologia analogiczna do tej z Eksperymentu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_fit_pairs = list(zip(inputs, best_fitnesses))\n",
    "for i in range(len(inputs)):\n",
    "    if i == 0:\n",
    "        print(\"Small Inputs:\")\n",
    "    elif i == 3:\n",
    "        print(\"Medium Inputs:\")\n",
    "    elif i == 6:\n",
    "        print(\"Large Inputs:\")\n",
    "    print(f\"    MLP{i} - fitness: {inp_fit_pairs[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hiperparams():\n",
    "    n_embd = np.random.choice([8, 16, 32])\n",
    "    n_hidden = np.random.choice(range(16, 81, 1))\n",
    "    norm_layer_num = np.random.choice([0, 1])\n",
    "    activation_num = np.random.choice([0, 1, 2])\n",
    "    lr = np.random.choice([0.0001, 0.001, 0.005, 0.01, 0.1])\n",
    "    batch_size = np.random.choice([16, 32, 64])\n",
    "    optimizer_num = np.random.choice([0, 1])\n",
    "    return n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mały input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallRandomMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = draw_hiperparams()\n",
    "rand_train_time, rand_step, rand_loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, SmallRandomMLP)\n",
    "\n",
    "train_time, step, loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, MLP7)\n",
    "\n",
    "print(f\"Random MLP training time: {rand_train_time} seconds\")\n",
    "print(f\"MLP7 training time: {train_time} seconds\")\n",
    "plt.plot(rand_step, rand_loss, label='Random MLP');\n",
    "plt.plot(step, loss, label='MLP7');\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Dev Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Średni input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumRandomMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = draw_hiperparams()\n",
    "rand_train_time, rand_step, rand_loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, MediumRandomMLP)\n",
    "\n",
    "train_time, step, loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, MLP4)\n",
    " \n",
    "print(f\"Random MLP training time: {rand_train_time} seconds\")\n",
    "print(f\"MLP4 training time: {train_time} seconds\")\n",
    "plt.plot(rand_step, rand_loss, label='Random MLP');\n",
    "plt.plot(step, loss, label='MLP4');\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Dev Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duży input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigRandomMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_hidden, bias, norm_layer, activation):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, n_hidden, bias=bias), norm_layer(n_hidden), activation,\n",
    "            nn.Linear(n_hidden, vocab_size, bias=bias), norm_layer(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "\n",
    "n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num = draw_hiperparams()\n",
    "rand_train_time, rand_step, rand_loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, BigRandomMLP)\n",
    "\n",
    "train_time, step, loss = training_loop(n_embd, n_hidden, norm_layer_num, activation_num, lr, batch_size, optimizer_num, MLP1)\n",
    "\n",
    "print(f\"Random MLP training time: {rand_train_time} seconds\")\n",
    "print(f\"MLP1 training time: {train_time} seconds\")\n",
    "plt.plot(rand_step, rand_loss, label='Random MLP');\n",
    "plt.plot(step, loss, label='MLP1');\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Dev Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski z Eksperymentu2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kilka próbek z modelu o najlepszych hiperparametrach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "activation = act_map[activation_num]\n",
    "norm_layer = norm_map[norm_layer_num]\n",
    "bias = True if int(norm_layer_num) == 1 else False\n",
    "model = MLP8(vocab_size, n_embd, n_hidden, bias, norm_layer, activation)\n",
    "optimizer = optim_map[optimizer_num](params=model.parameters(), lr=lr)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "step_arr, dev_loss_arr = [], []\n",
    "steps = 30000\n",
    "start = time.time()\n",
    "for step in range(steps):\n",
    "    if step == 18000:\n",
    "        optimizer.param_groups[0]['lr'] = lr * 0.7\n",
    "    elif step == 26000:\n",
    "        optimizer.param_groups[0]['lr'] = lr * 0.5\n",
    "\n",
    "    inx = torch.randint(0, training_set.shape[0], (int(batch_size),), generator=g)\n",
    "    emb = training_set[inx] # grab only those rows from the minibatch \n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # forward pass\n",
    "    preds = model(emb)\n",
    "    loss = loss_function(preds, y_training_set[inx])\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #stats\n",
    "    if step % 250 == 0:\n",
    "        dev_loss = get_loss(model, 'dev')\n",
    "        dev_loss_arr.append(dev_loss)\n",
    "        step_arr.append(step)\n",
    "\n",
    "end = time.time()\n",
    "training_time = round(end - start, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "model.eval()\n",
    "@torch.no_grad()\n",
    "def sample(model):\n",
    "\tg = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\tfor _ in range(20):\n",
    "\t\tout = '.' * block_size\n",
    "\t\twhile True:\n",
    "\t\t\tx = torch.tensor([str_to_inx[x] for x in out[-block_size:]])\n",
    "\t\t\tx = x.view(1, -1)\n",
    "\t\t\tlogits = model(x)\n",
    "\t\t\tprobs = F.softmax(logits, dim=1)\n",
    "\t\t\tinx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "\t\t\tout = out + inx_to_str[inx]\n",
    "\t\t\tif inx == 0:\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\tprint(out.replace('.', ''))\n",
    "sample(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
