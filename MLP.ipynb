{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "str_to_inx = {str:inx for inx, str in enumerate(chars, start=1)}\n",
    "str_to_inx['.'] = 0\n",
    "inx_to_str = {str:inx for inx, str in str_to_inx.items()}\n",
    "vocab_size = len(inx_to_str)\n",
    "\n",
    "# build the dataset\n",
    "random.shuffle(words)\n",
    "block_size = 4 # context length: how many characters do we take to predict the next one?\n",
    "X, Y = [], []\n",
    "for word in words:\n",
    "    word = block_size * '.' + word + '.'\n",
    "    end_inx = block_size\n",
    "    for start_inx, char in enumerate(word[block_size:]):\n",
    "        X.append([str_to_inx[ch] for ch in word[start_inx:end_inx]])\n",
    "        Y.append(str_to_inx[char])\n",
    "        end_inx += 1\n",
    "\n",
    "\n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "X.shape, Y.shape\n",
    "# data set splits 80%, 10%, 10%\n",
    "train_range = math.ceil(len(X) * 0.8)\n",
    "dev_range = (len(X) - train_range) // 2\n",
    "\n",
    "training_set = X[:train_range]\n",
    "dev_set = X[train_range:train_range+dev_range]\n",
    "test_set = X[train_range+dev_range:]\n",
    "\n",
    "y_training_set = Y[:train_range]\n",
    "y_dev_set = Y[train_range:train_range+dev_range]\n",
    "y_test_set = Y[train_range+dev_range:]\n",
    "\n",
    "assert training_set.nelement() + dev_set.nelement() + test_set.nelement() == X.nelement(), \"Bad split\"\n",
    "\n",
    "device = \"cpu\"\n",
    "n_embd = 10 # dim of the character embedding vector\n",
    "n_hidden = 100 # the # of neurons in the hidden layer\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.linear_tanh_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "            nn.Linear(n_embd * block_size, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            nn.Linear(n_hidden, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_tanh_stack(x)\n",
    "        return logits\n",
    "model = MLP(vocab_size=vocab_size, n_embd=n_embd).to(device)\n",
    "print(model)\n",
    "losses, inxs = [], []\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "weight_decay = 1e-3\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "steps = 100000\n",
    "model.train()\n",
    "for epoch in range(steps):\n",
    "    inx = torch.randint(0, training_set.shape[0], (32,), generator=g)\n",
    "    emb = training_set[inx] # grab only those rows from the minibatch \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    preds = model(emb)\n",
    "    loss = loss_function(preds, y_training_set[inx])\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #stats\n",
    "    if epoch % 1000 == 0: # print every once in a while\n",
    "        print(f'{epoch:7d}/{steps:7d}: {loss.item():.4f}')\n",
    "    losses.append(loss.item())\n",
    "    inxs.append(epoch)\n",
    "   \n",
    "# plot loss\n",
    "plt.plot(inxs, np.log10(losses))\n",
    "# plt.xscale(\"log\")\n",
    "model.eval()\n",
    "@torch.no_grad() # disable gradient tracking\n",
    "def get_loss(model, data_set):\n",
    "    x, y = {\n",
    "        'train': (training_set, y_training_set),\n",
    "        'dev': (dev_set, y_dev_set),\n",
    "        'test': (test_set, y_test_set)\n",
    "    }[data_set]\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(data_set, loss.item())\n",
    "\n",
    "get_loss(model,'train')\n",
    "get_loss(model,'dev')\n",
    "# sample from the model\n",
    "model.eval()\n",
    "@torch.no_grad()\n",
    "def sample(model):\n",
    "\tg = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\tfor _ in range(20):\n",
    "\t\tout = '.' * block_size\n",
    "\t\twhile True:\n",
    "\t\t\tx = torch.tensor([str_to_inx[x] for x in out[-block_size:]])\n",
    "\t\t\tx = x.view(1, -1)\n",
    "\t\t\tlogits = model(x)\n",
    "\t\t\tprobs = F.softmax(logits, dim=1)\n",
    "\t\t\tinx = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "\t\t\tout = out + inx_to_str[inx]\n",
    "\t\t\tif inx == 0:\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\tprint(out.replace('.', ''))\n",
    "sample(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
